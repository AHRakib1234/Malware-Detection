
import os
import json
from collections import deque
from itertools import islice
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer


def extract_api_calls(report_path):
    api_calls = []

    with open(report_path, 'r') as f:
        report = json.load(f)

    if 'behavior' in report and 'processes' in report['behavior']:
        for process in report['behavior']['processes']:
            if 'calls' in process:
                for call in process['calls']:
                    if 'api' in call:
                        api_calls.append(call['api'])

    return api_calls

def generate_ngrams(input_list, n):
    ngrams = zip(*(islice(input_list, i, None) for i in range(n)))
    return [' '.join(ngram) for ngram in ngrams]

def process_reports(directory, label, ngram_file):
    features = []

    try:
        for filename in os.listdir(directory):
            report_path = os.path.join(directory, filename)
            api_calls = extract_api_calls(report_path)
            ngram_features = generate_ngrams(api_calls, n=5)  # Adjust n-gram size as needed
            features.extend(ngram_features)
    except:
        pass

    # Save n-gram features to file
    with open(ngram_file, 'w') as f:
        for feature in features:
            f.write("%s\n" % feature)

    return [label] * len(features)

malware_dir = 'path'
benign_dir = 'path'
#
# # Process malware reports and save n-gram features
malware_labels = process_reports(malware_dir, label='malicious', ngram_file='malware_ngrams.txt')
#
# # Process benign reports and save n-gram features
benign_labels = process_reports(benign_dir, label='benign', ngram_file='benign_ngrams.txt')

# Combine features and labels
# all_labels = malware_labels + benign_labels

with open('malware_ngrams.txt', 'r') as f:
    malware_features = f.read().splitlines()

with open('benign_ngrams.txt', 'r') as f:
    benign_features = f.read().splitlines()

malware_features = list(set(malware_features))
benign_features = list(set(benign_features))

all_features = malware_features + benign_features
all_labels = ['malicious'] * len(malware_features) + ['benign'] * len(benign_features)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.2, random_state=42)

# Convert text data into numerical feature vectors
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train classifiers with TF-IDF features
classifiers = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='linear', random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

for clf_name, clf in classifiers.items():
    print(f"Training {clf_name}...")
    clf.fit(X_train_tfidf, y_train)

    # Save trained model
    joblib.dump(clf, f"{clf_name}_model_tfidf.pkl")

    # Predictions
    y_pred = clf.predict(X_test_tfidf)

    # Calculate evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, pos_label='malicious')
    recall = recall_score(y_test, y_pred, pos_label='malicious')
    f1 = f1_score(y_test, y_pred, pos_label='malicious')

    # Print evaluation metrics
    print(f"{clf_name} Metrics with TF-IDF:")
    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1-score:", f1)
    print()
